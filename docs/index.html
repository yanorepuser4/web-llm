<!DOCTYPE html>
<html>
  <head>
    <title>WebLLM | Home</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css"
      integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
    />
    <link rel="stylesheet" href="/assets/css/main.css" />
    <link rel="stylesheet" href="/assets/css/group.css" />
    <!-- <link rel="stylesheet" href="/css/table.css">          -->
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="/assets/img/logo/mlc-favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="/assets/img/logo/mlc-favicon-16x16.png"
    />
    <link
      rel="icon"
      type="image/x-icon"
      sizes="48x48"
      href="/assets/img/logo/mlc-favicon.ico"
    />
    <meta name="description" content="WebLLM: High-Performance In-Browser LLM Inference Engine">
<meta
  http-equiv="origin-trial"
  content="Agx76XA0ITxMPF0Z8rbbcMllwuxsyp9qdtQaXlLqu1JUrdHB6FPonuyIKJ3CsBREUkeioJck4nn3KO0c0kkwqAMAAABJeyJvcmlnaW4iOiJodHRwOi8vbG9jYWxob3N0Ojg4ODgiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5fQ=="
/>
<meta
  http-equiv="origin-trial"
  content="AnmwqQ1dtYDQTYkZ5iMtHdINCaxjE94uWQBKp2yOz1wPTcjSRtOHUGQG+r2BxsEuM0qhxTVnuTjyh31HgTeA8gsAAABZeyJvcmlnaW4iOiJodHRwczovL21sYy5haTo0NDMiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZX0="
/>
<script src="https://code.jquery.com/jquery-3.6.3.min.js" integrity="sha256-pvPw+upLPUjgMXY0G+8O0xUf+/Im1MZjXxxgOcBQBXU=" crossorigin="anonymous"></script>
<link rel="stylesheet" href="/assets/css/hero.css" />
  </head>
  <body>
    <div class="container">
      <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
      <div
        class="header d-flex flex-column flex-md-row justify-content-md-between"
      >
        <a href="/" id="navtitle">
          <img
            src="/assets/img/logo/mlc-logo-with-text-landscape.svg"
            height="70px"
            alt="MLC"
            id="logo"
          />
        </a>
        <ul id="navbar" class="nav nav-pills justify-content-center">
               

          <li class="nav-item">
            
            <a
              class="nav-link active"
              href="/"
            >
              Home
            </a>
            
          </li>

              

          <li class="nav-item">
            
            <a class="nav-link " href="https://github.com/mlc-ai/web-llm">
              GitHub
            </a>
            
          </li>

          
        </ul>
      </div>

       
      <!-- Schedule  -->
      
      <section id="hero">
  <div class="heading-container">
    <h1>WebLLM: High-Performance In-Browser LLM Inference Engine</h1>
    <div class="link-container">
      <a class="get-start-link" href="https://github.com/mlc-ai/web-llm?tab=readme-ov-file#get-started">
        <span class="get-start-link-content">
          <span>Get Started</span>
          <span class="arrow-container"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" aria-hidden="true" focusable="false" data-testid="Button-expandable-arrow">
  <path fill="currentColor" d="M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"></path>
  <path class="arrow-expandable" stroke="currentColor" d="M1.75 8H11" stroke-width="1.5" stroke-linecap="round"></path>
</svg>
</span></span>
      </a>
      <a class="chat-link moving-border" href="https://chat.webllm.ai">
        <span class="border"></span>
        <span class="chat-link-content">
          <span>Chat with WebLLM</span>
          <span class="arrow-container"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" aria-hidden="true" focusable="false" data-testid="Button-expandable-arrow">
  <path fill="currentColor" d="M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"></path>
  <path class="arrow-expandable" stroke="currentColor" d="M1.75 8H11" stroke-width="1.5" stroke-linecap="round"></path>
</svg>
</span>
          </span>
      </a>
    </div>
  </div>
  <div class="video-container">
    <video autoplay="" playsinline="" muted="" poster="/assets/img/fig/light_demo.png">
      <source src="/assets/video/light_demo.webm" type="video/webm" />
      <source src="/assets/video/light_demo.mp4" type="video/mp4" />
    </video>
  </div>
</section>

<script>
  (function() {

  function handlerIn(e) {
    $(this).addClass("expanded");
  }
  function handlerOut(e) {
    $(this).removeClass("expanded");
  }

  $(".chat-link").hover(handlerIn, handlerOut);
  $(".github-link").hover(handlerIn, handlerOut);

  var video = $("video")[0];
  video.play().then((_) => {
    let observer = new IntersectionObserver(
      (entries) => {
        entries.forEach((entry) => {
          if (
              entry.intersectionRatio !== 1 &&
              !video.paused
          ) {
            video.pause();
          } else if (video.paused) {
            video.play();
          }
        });
      },
        { threshold: 0.2 }
    );
    observer.observe(video);
  });
})()
</script>

<h2 id="overview">Overview</h2>

<p>We have been seeing amazing progress in generative AI and LLM recently. Thanks to the open-source efforts like LLaMA, Alpaca, Vicuna and Dolly, we start to see an exciting future of building our own open source language models and personal AI assistant.</p>

<p>These models are usually big and compute-heavy. To build a chat service, we will need a large cluster to run an inference server, while clients send requests to servers and retrieve the inference output. We also usually have to run on a specific type of GPUs where popular deep-learning frameworks are readily available.</p>

<p>This project is our step to bring more diversity to the ecosystem. Specifically, can we simply bake LLMs directly into the client side and directly run them inside a browser? If that can be realized, we could offer support for client personal AI models with the benefit of cost reduction, enhancement for personalization and privacy protection. The client side is getting pretty powerful.</p>

<p>Won’t it be even more amazing if we can simply open up a browser and directly bring AI natively to your browser tab? There is some level of readiness in the ecosystem. This project provides an affirmative answer to the question.</p>

<h2 id="key-features">Key Features</h2>
<ul>
  <li>
    <p><strong>In-Browser Inference</strong>: WebLLM is a high-performance, in-browser language model inference engine that leverages WebGPU for hardware acceleration, enabling powerful LLM operations directly within web browsers without server-side processing.</p>
  </li>
  <li>
    <p><a href="https://github.com/mlc-ai/web-llm?tab=readme-ov-file#full-openai-compatibility"><strong>Full OpenAI API Compatibility</strong></a>: Seamlessly integrate your app with WebLLM using OpenAI API with functionalities such as JSON-mode, function-calling, streaming, and more.</p>
  </li>
  <li>
    <p><a href="https://github.com/mlc-ai/web-llm?tab=readme-ov-file#built-in-models"><strong>Extensive Model Support</strong></a>: WebLLM natively supports a range of models including Llama, Phi, Gemma, RedPajama, Mistral, Qwen(通义千问), and many others, making it versatile for various AI tasks.</p>
  </li>
  <li>
    <p><a href="https://github.com/mlc-ai/web-llm?tab=readme-ov-file#custom-models"><strong>Custom Model Integration</strong></a>: Easily integrate and deploy custom models in MLC format, allowing you to adapt WebLLM to specific needs and scenarios, enhancing flexibility in model deployment.</p>
  </li>
  <li>
    <p><strong>Plug-and-Play Integration</strong>: Easily integrate WebLLM into your projects using package managers like NPM and Yarn, or directly via CDN, complete with comprehensive <a href="https://github.com/mlc-ai/web-llm/tree/main/examples">examples</a> and a modular design for connecting with UI components.</p>
  </li>
  <li>
    <p><strong>Streaming &amp; Real-Time Interactions</strong>: Supports streaming chat completions, allowing real-time output generation which enhances interactive applications like chatbots and virtual assistants.</p>
  </li>
  <li>
    <p><strong>Web Worker &amp; Service Worker Support</strong>: Optimize UI performance and manage the lifecycle of models efficiently by offloading computations to separate worker threads or service workers.</p>
  </li>
  <li>
    <p><strong>Chrome Extension Support</strong>: Extend the functionality of web browsers through custom Chrome extensions using WebLLM, with examples available for building both basic and advanced extensions.</p>
  </li>
</ul>

<h2 id="disclaimer">Disclaimer</h2>

<p>The <a href="https://chat.webllm.ai">demo site</a> is for research purposes only, subject to the model License of LLaMA, Vicuna and RedPajama. Please contact us if you find any potential violation.</p>
 
    </div>
    <!-- /container -->

    <!-- Support retina images. -->
    <script
      type="text/javascript"
      src="/assets/js/srcset-polyfill.js"
    ></script>
  </body>
</html>
